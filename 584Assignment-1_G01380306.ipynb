{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=1000)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "import nltk\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from numpy.linalg import norm\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial.distance import cityblock\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training Data path -> \"/Users/sanjanagovindu/Downloads/DM1/train_file.csv\"\n",
    "#Test Data path -> \"/Users/sanjanagovindu/Downloads/DM1/test_file.csv\"\n",
    "\n",
    "train=pd.read_fwf(\"/Users/sanjanagovindu/Downloads/DM1/train_file.csv\",colspecs=[(0,2),(3,None)],names=[\"sentiment\", \"review\"],delimiter=\"#EOF\")\n",
    "test=pd.read_fwf(\"/Users/sanjanagovindu/Downloads/DM1/test_file.csv\",names=[\"review\"],delimiter=\"\\n\")\n",
    "\n",
    "train_sentiments=train[\"sentiment\"].tolist()\n",
    "train_reviews=train[\"review\"].tolist()\n",
    "test_reviews=test[\"review\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    \n",
    "    #Convert data to lower case\n",
    "    df['cleanReview'] = df['review'].str.lower() \n",
    "    #Remove punctuations\n",
    "    df['cleanReview'] = df['cleanReview'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "    #Remove numbers\n",
    "    df['cleanReview'] = df['cleanReview'].str.replace('\\d+', '')\n",
    "    #Remove HTML Tags\n",
    "    df['cleanReview'] = df['cleanReview'].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "    #Remove URLs\n",
    "    df['cleanReview'] = df['cleanReview'].str.replace('https?://S+|www.S+', '', regex=True)\n",
    "    #Remove white spaces\n",
    "    df['cleanReview'] = df['cleanReview'].str.strip()\n",
    "    #Spelling Correction\n",
    "    \n",
    "    #Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['cleanReview'] = df['cleanReview'].apply(lambda x: ' '.join([w for w in x.split() if w not in (stop_words)]))\n",
    "    \n",
    "    #Split words\n",
    "    df['cleanReview'] = df['cleanReview'].str.split()\n",
    "    \n",
    "    #Tokenization\n",
    "#     df['cleanReview'] = df.apply(lambda row: nltk.word_tokenize(row['cleanReview']), axis=1)\n",
    "    \n",
    "    #Stemming\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    df['cleanReview'] = df['cleanReview'].apply(lambda x: [porter_stemmer.stem(y) for y in x])\n",
    "    \n",
    "    #Lemmetization\n",
    "#     lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "#     df['cleanReview'] = df['cleanReview'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "    \n",
    "    cleanedData=df['cleanReview'].tolist()\n",
    "    for i in range(0,len(cleanedData)):\n",
    "        cleanedData[i]=' '.join(cleanedData[i])\n",
    "    return cleanedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(nearest_neighbors, sentiments):    \n",
    "    pos= 0\n",
    "    neg= 0\n",
    "    for neighbor in nearest_neighbors:\n",
    "        if int(sentiments[neighbor]) == 1:\n",
    "            pos+= 1\n",
    "        else:\n",
    "            neg+= 1\n",
    "    #Calculate max of postives and negatives to predict the test sentiments\n",
    "    if max(pos,neg)==pos:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateDistance(test_vector,train_vector):\n",
    "    #Cosine Similarity\n",
    "    distance=cosine_similarity(test_vector,train_vector)\n",
    "    \n",
    "     #Euclidean distance\n",
    "#     distance=euclidean_distances(test_vector,train_vector)\n",
    "\n",
    "     #Manhattan distance\n",
    "#     distance = cityblock(test_vector,train_vector)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(train_vector,test_vector,train_sentiments,k):\n",
    "    \n",
    "    #Calculate distance - Cosine similarity\n",
    "    distance=calculateDistance(test_vector,train_vector)\n",
    "\n",
    "    test_sentiments = []\n",
    "    for d in distance:\n",
    "        #returns kNN indices\n",
    "        knn = np.argsort(d)[::-1][:k]\n",
    "        prediction = predict(knn, train_sentiments)\n",
    "        test_sentiments.append(1) if prediction == 1 else test_sentiments.append(-1)\n",
    "\n",
    "    return test_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinAll(arr):\n",
    "    for i in range(0,len(arr)):\n",
    "        arr[i]=' '.join(arr[i])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing both training data and testing data to create simarity vector\n",
    "train_reviews = preprocessing(train)\n",
    "test_reviews = preprocessing(test)\n",
    "\n",
    "#TFIDF Vectorization - converts training and test data list to vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vector = vectorizer.fit_transform(train_reviews)\n",
    "test_vector = vectorizer.transform(test_reviews)\n",
    "\n",
    "#SVD (Singular value decomposition) - Factorization of a matrix.\n",
    "# train_vector=svd.fit_transform(train_vector)\n",
    "# test_vector=svd.transform(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of each fold - [0.8478924862553452, 0.8485033598045205, 0.8625534514355528, 0.8417837507635919, 0.8618581907090465, 0.8575794621026895, 0.863080684596577, 0.8374083129584352, 0.8325183374083129, 0.8459657701711492, 0.8533007334963325]\n",
      "Avg accuracy : 0.8502222308819595\n"
     ]
    }
   ],
   "source": [
    "#K-Fold Cross Validation\n",
    "kfold = KFold(n_splits=11, random_state=None)\n",
    "x=train['cleanReview']\n",
    "y=train['sentiment']\n",
    "accuracies = []\n",
    " \n",
    "for train_index , test_index in kfold.split(x):\n",
    "    x_train , x_test = x.iloc[train_index],x.iloc[test_index]\n",
    "    y_train , y_test = y[train_index] , y[test_index]\n",
    "    x_train=x_train.tolist()\n",
    "    y_train=y_train.tolist()\n",
    "    x_test=x_test.tolist()\n",
    "    \n",
    "    train_reviews1, test_reviews1=joinAll(x_train),joinAll(x_test)\n",
    "    \n",
    "    #TFIDF Vectorization\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_vector1 = vectorizer.fit_transform(train_reviews1)\n",
    "    test_vector1 = vectorizer.transform(test_reviews1)\n",
    "    \n",
    "    a = accuracy_score(pd.Series(KNN(train_vector1,test_vector1,y_train,255)) , y_test)\n",
    "    accuracies.append(a)\n",
    "\n",
    "print('Accuracy of each fold - {}'.format(accuracies))\n",
    "avg_accuracy = sum(accuracies)/k\n",
    "print('Avg accuracy : {}'.format(avg_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiments=KNN(train_vector,test_vector,train_sentiments,255)\n",
    "#Writing the output which is test sentiments to an output text file\n",
    "fout = open('/Users/sanjanagovindu/Downloads/DM1/output.txt', 'w')\n",
    "fout.writelines( \"%s\\n\" % i for i in test_sentiments)\n",
    "fout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
